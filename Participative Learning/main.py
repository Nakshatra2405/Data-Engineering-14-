# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1djdt8jPdt_fvGuDLVAo4V5GdM6n0pHk7
"""

!pip -q install pyspark==3.5.1 openpyxl reportlab

from pyspark.sql import SparkSession
import pandas as pd, os, matplotlib.pyplot as plt
spark = SparkSession.builder.appName("PlayStoreAnalysis").getOrCreate()

# Upload your Excel
from google.colab import files
up = files.upload('ola_benglore_rides.csv')
csv_name = list(up.keys())[0]

csv_path = "ola_benglore_rides.csv"

from pyspark.sql.types import *

schema = StructType([
    StructField("Trip_ID", StringType(), True),
    StructField("Pickup_DateTime", TimestampType(), True),
    StructField("Drop_DateTime", TimestampType(), True),
    StructField("Pickup_Location", StringType(), True),
    StructField("Drop_Location", StringType(), True),
    StructField("City", StringType(), True),
    StructField("Fare_Amount", DoubleType(), True),
    StructField("Payment_Type", StringType(), True),
])

df_raw = spark.read.option("header", True).schema(schema).csv(csv_path)
df_raw.printSchema()
df_raw.show(5, truncate=False)

from pyspark.sql import functions as F

df = (
    df_raw
    # Cast Fare_Amount safely
    .withColumn("fare_amount_num", F.col("Fare_Amount").cast("double"))

    # Normalize Payment_Type
    .withColumn("payment_type_norm", F.initcap(F.trim(F.col("Payment_Type"))))

    # Normalize City
    .withColumn("city_norm", F.trim(F.col("City")))

    # Keep only valid Trip_ID
    .filter(F.col("Trip_ID").isNotNull())

    # Keep rows where either:
    #   1. Trip was completed (Pickup & Drop time present, fare >= 0)
    #   2. Trip was cancelled/incomplete (Drop or Fare may be NULL, but keep for analysis)
    .filter(
        (F.col("Pickup_DateTime").isNotNull()) &
        (
            (F.col("Drop_DateTime").isNotNull() & (F.col("fare_amount_num") >= 0)) |
            (F.col("Drop_DateTime").isNull())  # keep cancelled/incomplete
        )
    )
)

# Cache and count
df.cache()
print("Rows after cleaning:", df.count())

from pyspark.sql import functions as F

# 1) Top 10 trips by fare
top10_fare = (
    df.orderBy(F.desc("fare_amount_num"))
      .limit(10)
)

# 2) Trip count by payment type
payment_dist_count = (
    df.groupBy("payment_type_norm")
      .agg(F.count("*").alias("trip_count"))
)

# 2b) Total fare by payment type
payment_dist_fare = (
    df.groupBy("payment_type_norm")
      .agg(F.sum("fare_amount_num").alias("total_fare"))
)

# 3) City-wise stats
city_summary = (
    df.groupBy("city_norm")
      .agg(
          F.count("*").alias("num_trips"),
          F.sum("fare_amount_num").alias("total_fare"),
          F.avg("fare_amount_num").alias("avg_fare")
      )
      .orderBy(F.desc("num_trips"))
)

# 4) Top pickup locations by trips
top_pickup_locations = (
    df.groupBy("Pickup_Location")
      .agg(F.count("*").alias("num_trips"))
      .orderBy(F.desc("num_trips"))
      .limit(10)
)

# 5) Top drop locations by trips
top_drop_locations = (
    df.groupBy("Drop_Location")
      .agg(F.count("*").alias("num_trips"))
      .orderBy(F.desc("num_trips"))
      .limit(10)
)

# 6) Average fare by payment type (only completed trips)
avg_fare_payment = (
    df.filter(F.col("Drop_DateTime").isNotNull())
      .groupBy("payment_type_norm")
      .agg(F.avg("fare_amount_num").alias("avg_fare"))
)

# Quick peek in notebook
for title, sdf in [
    ("Top 10 Trips by Fare", top10_fare),
    ("Trip Count by Payment Type", payment_dist_count),
    ("Total Fare by Payment Type", payment_dist_fare),
    ("City-wise Summary", city_summary),
    ("Top Pickup Locations", top_pickup_locations),
    ("Top Drop Locations", top_drop_locations),
    ("Average Fare by Payment Type", avg_fare_payment),
]:
    print("====", title, "====")
    sdf.show(truncate=False)

import os
import matplotlib.pyplot as plt

fig_dir = "/content/figs"
os.makedirs(fig_dir, exist_ok=True)

def barh_plot(sdf, x, y, title, fname):
    _pdf = sdf.toPandas()
    _pdf[y] = _pdf[y].fillna("Unknown").astype(str)
    plt.figure(figsize=(10,6))
    plt.barh(_pdf[y], _pdf[x])
    plt.gca().invert_yaxis()
    plt.title(title)
    plt.tight_layout()
    out = f"{fig_dir}/{fname}"
    plt.savefig(out, dpi=150)
    plt.close()
    return out

charts = {
    "Top 10 Trips by Fare": barh_plot(
        top10_fare, "fare_amount_num", "Trip_ID",
        "Top 10 Trips by Fare", "topfare.png"
    ),
    "Trip Count by Payment Type": barh_plot(
        payment_dist_count, "trip_count", "payment_type_norm",
        "Trip Count by Payment Type", "paymentcount.png"
    ),
    "Total Fare by Payment Type": barh_plot(
        payment_dist_fare, "total_fare", "payment_type_norm",
        "Total Fare by Payment Type", "paymentfare.png"
    ),
    "City-wise Summary (Top 10 by Trips)": barh_plot(
        city_summary.limit(10), "num_trips", "city_norm",
        "City-wise Trips", "cities.png"
    ),
    "Top Pickup Locations": barh_plot(
        top_pickup_locations, "num_trips", "Pickup_Location",
        "Top Pickup Locations", "pickup.png"
    ),
    "Top Drop Locations": barh_plot(
        top_drop_locations, "num_trips", "Drop_Location",
        "Top Drop Locations", "drop.png"
    ),
    "Average Fare by Payment Type": barh_plot(
        avg_fare_payment, "avg_fare", "payment_type_norm",
        "Average Fare by Payment Type", "avgfare.png"
    ),
}

charts

from pyspark.sql import functions as F
from pyspark.sql.window import Window

insights = {}

# 1) Totals
insights["total_trips"] = df.count()
insights["total_fare"] = (
    df.agg(F.sum("fare_amount_num").alias("s")).first()["s"]
)

# Cash share
row_cash = (
    ptc.filter(F.col("payment_type_norm") == "Cash")
       .select((F.col("trip_count_d") / tot_trips).alias("share"))
       .first()
)
insights["cash_share_trips"] = float(row_cash["share"]) if row_cash and row_cash["share"] is not None else 0.0

# Online share
row_online = (
    ptc.filter(F.col("payment_type_norm") == "Online")
       .select((F.col("trip_count_d") / tot_trips).alias("share"))
       .first()
)
insights["online_share_trips"] = float(row_online["share"]) if row_online and row_online["share"] is not None else 0.0

# 3) Top city by trips
top_city_row = city_summary.select("city_norm", "num_trips").first()
if top_city_row:
    insights["top_city"] = top_city_row["city_norm"] or "Unknown"
    insights["top_city_trips"] = int(top_city_row["num_trips"])

# 4) Average fare: Cash vs Online
avg_by_payment = (
    df.groupBy("payment_type_norm")
      .agg(F.avg("fare_amount_num").alias("avg_fare"))
)

insights["avg_fare_cash"] = avg_by_payment.filter(F.col("payment_type_norm") == "Cash").select("avg_fare").first()
insights["avg_fare_cash"] = float(insights["avg_fare_cash"][0]) if insights["avg_fare_cash"] else None

insights["avg_fare_online"] = avg_by_payment.filter(F.col("payment_type_norm") == "Online").select("avg_fare").first()
insights["avg_fare_online"] = float(insights["avg_fare_online"][0]) if insights["avg_fare_online"] else None

# 5) Best (highest fare) trip
best_trip = (
    df.orderBy(F.desc("fare_amount_num"))
      .select("Trip_ID", "City", "fare_amount_num", "payment_type_norm", "Pickup_Location", "Drop_Location")
      .first()
)

if best_trip:
    insights["highest_fare_trip"] = dict(
        trip_id = best_trip["Trip_ID"],
        city = best_trip["City"],
        fare = float(best_trip["fare_amount_num"]) if best_trip["fare_amount_num"] is not None else None,
        payment_type = best_trip["payment_type_norm"] or "Unknown",
        pickup = best_trip["Pickup_Location"] or "Unknown",
        drop = best_trip["Drop_Location"] or "Unknown"
    )

# 6) Simple correlation: fare vs trip duration (for completed trips)
completed = df.filter(F.col("Drop_DateTime").isNotNull()) \
              .withColumn("duration_min",
                          (F.unix_timestamp("Drop_DateTime") - F.unix_timestamp("Pickup_DateTime"))/60.0) \
              .select("fare_amount_num", "duration_min").dropna()

corr = None
try:
    if completed.count() >= 3:
        corr = completed.stat.corr("fare_amount_num", "duration_min")
except Exception:
    corr = None

insights["corr_fare_duration"] = corr

insights

!pip install reportlab

from pyspark.sql import functions as F
import os
import matplotlib.pyplot as plt

# -------------------------
# 1) Prepare aggregated DataFrames
# -------------------------
# Top 10 Pickup Locations
top_pickup_df = (
    df_raw.groupBy("Pickup_Location")
          .agg(F.count("*").alias("trip_count"))
          .orderBy(F.desc("trip_count"))
          .limit(10)
)

# Top 10 Drop Locations
top_drop_df = (
    df_raw.groupBy("Drop_Location")
          .agg(F.count("*").alias("trip_count"))
          .orderBy(F.desc("trip_count"))
          .limit(10)
)

# Trips by City
city_dist_df = (
    df_raw.groupBy("City")
          .agg(
              F.count("*").alias("num_trips"),
              F.sum(F.col("Fare_Amount").cast("double")).alias("total_fare")
          )
          .orderBy(F.desc("num_trips"))
)

# Payment type distribution
payment_dist_df = (
    df_raw.groupBy("Payment_Type")
          .agg(F.count("*").alias("num_trips"))
)

# Top 10 trips by Fare
top_fare_df = (
    df_raw.withColumn("fare_num", F.col("Fare_Amount").cast("double"))
          .orderBy(F.desc("fare_num"))
          .limit(10)
)

# -------------------------
# 2) Generate bar charts
# -------------------------
fig_dir = "/content/figs"
os.makedirs(fig_dir, exist_ok=True)

def barh_plot(sdf, x, y, title, fname):
    _pdf = sdf.toPandas()
    _pdf[y] = _pdf[y].fillna("Unknown").astype(str)
    plt.figure(figsize=(10,6))
    plt.barh(_pdf[y], _pdf[x])
    plt.gca().invert_yaxis()
    plt.title(title)
    plt.tight_layout()
    out = f"{fig_dir}/{fname}"
    plt.savefig(out, dpi=150)
    plt.close()
    return out

charts = {
    "Top 10 Pickup Locations": barh_plot(top_pickup_df, "trip_count", "Pickup_Location", "Top 10 Pickup Locations", "pickup.png"),
    "Top 10 Drop Locations": barh_plot(top_drop_df, "trip_count", "Drop_Location", "Top 10 Drop Locations", "drop.png"),
    "Trips by City": barh_plot(city_dist_df.limit(10), "num_trips", "City", "Trips by City", "city.png"),
    "Payment Type Distribution": barh_plot(payment_dist_df, "num_trips", "Payment_Type", "Payment Type Distribution", "payment.png"),
    "Top 10 Trips by Fare": barh_plot(top_fare_df, "fare_num", "Trip_ID", "Top 10 Trips by Fare", "fare.png"),
}

# -------------------------
# 3) Insights dictionary
# -------------------------
insights = {}
insights["total_trips"] = df_raw.count()
insights["total_fare"] = float(df_raw.select(F.sum(F.col("Fare_Amount").cast("double"))).first()[0] or 0)

# Cash / Online shares
tot_trips = insights["total_trips"] or 1
ptc = payment_dist_df.withColumn("trip_count_d", F.col("num_trips").cast("double"))
insights["cash_share"] = float(ptc.filter(F.col("Payment_Type")=="Cash").select((F.col("trip_count_d")/tot_trips)).first()[0] or 0)
insights["online_share"] = float(ptc.filter(F.col("Payment_Type")=="Online").select((F.col("trip_count_d")/tot_trips)).first()[0] or 0)

# Top city by trips
top_city_row = city_dist_df.select("City", "num_trips").first()
if top_city_row:
    insights["top_city"] = top_city_row["City"]
    insights["top_city_trips"] = int(top_city_row["num_trips"])

# Highest fare trip
hf = df_raw.withColumn("fare_num", F.col("Fare_Amount").cast("double")).orderBy(F.desc("fare_num")).first()
if hf:
    insights["highest_fare_trip"] = {
        "trip_id": hf["Trip_ID"],
        "fare": hf["Fare_Amount"],
        "pickup": hf["Pickup_Location"],
        "drop": hf["Drop_Location"],
        "city": hf["City"]
    }

from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Image, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.lib import colors

pdf_file = "/content/Trip_Report.pdf"

styles = getSampleStyleSheet()
H1, H2, H3, N = styles["Heading1"], styles["Heading2"], styles["Heading3"], styles["Normal"]

def spark_to_table(sdf, max_rows=10):
    if sdf.count() == 0:
        return Paragraph("No data available.", N)
    _pdf = sdf.limit(max_rows).toPandas().fillna("")
    data = [list(map(str, _pdf.columns))] + _pdf.astype(str).values.tolist()
    t = Table(data, repeatRows=1)
    t.setStyle(TableStyle([
        ("BACKGROUND", (0,0), (-1,0), colors.lightgrey),
        ("TEXTCOLOR", (0,0), (-1,0), colors.black),
        ("GRID", (0,0), (-1,-1), 0.25, colors.grey),
        ("FONTSIZE", (0,0), (-1,-1), 8),
        ("ALIGN", (1,1), (-1,-1), "LEFT"),
        ("VALIGN", (0,0), (-1,-1), "MIDDLE"),
    ]))
    return t

doc = SimpleDocTemplate(pdf_file, pagesize=A4)
story = []

# Title & Problem Statement
story += [
    Paragraph("Trip Analysis Report â€“ PySpark on Google Colab", H1),
    Spacer(1,6),
    Paragraph("<b>Problem Statement</b>", H2),
    Paragraph("""
Analyze the trip dataset using PySpark and present:
1) Top 10 busiest pickup locations
2) Top 10 busiest drop locations
3) City-wise distribution of trips
4) Payment type distribution (Cash vs Online)
5) Trips with highest fare amounts
""", N),
    Spacer(1,10)
]

# Steps
story += [
    Paragraph("Step 1: Load & Schema", H2),
    Paragraph("We load the trip CSV data into Spark with explicit schema to avoid type issues.", N),
    Spacer(1,8),
    Paragraph("Step 2: Clean & Normalize", H2),
    Paragraph("We filter out trips with null pickup/drop times or fares, normalize fare amounts.", N),
    Spacer(1,8),
    Paragraph("Step 3: Compute Aggregations", H2),
    Paragraph("We calculate top locations, city-wise summaries, payment type distribution, and high fare trips.", N),
    Spacer(1,10)
]

# Sections (replace these with your actual Spark DataFrames & chart paths)
sections = [
    ("Top 10 Pickup Locations", top_pickup_df, charts["Top 10 Pickup Locations"]),
    ("Top 10 Drop Locations", top_drop_df, charts["Top 10 Drop Locations"]),
    ("Trips by City", city_dist_df, charts["Trips by City"]),
    ("Payment Type Distribution", payment_dist_df, charts["Payment Type Distribution"]),
    ("Top 10 Trips by Fare", top_fare_df, charts["Top 10 Trips by Fare"]),
]

for title, sdf, chart_path in sections:
    story += [
        Paragraph(title, H2),
        spark_to_table(sdf, max_rows=10),
        Spacer(1,6),
        Image(chart_path, width=400, height=260),
        Spacer(1,10)
    ]

# Conclusions & Insights
def fmt_pct(x):
    return f"{x*100:.1f}%" if x is not None else "n/a"

story += [Paragraph("Conclusions & Insights", H2)]
cons_txt = []

cons_txt.append(f"Total trips analyzed: <b>{insights.get('total_trips','n/a')}</b>.")
if insights.get("total_fare") is not None:
    cons_txt.append(f"Total fare amount (sum): <b>{insights['total_fare']:,}</b>.")

cons_txt.append(
    f"Cash trips share: <b>{fmt_pct(insights.get('cash_share'))}</b>; "
    f"Online trips share: <b>{fmt_pct(insights.get('online_share'))}</b>."
)

if insights.get("top_city"):
    cons_txt.append(
        f"City with most trips: <b>{insights['top_city']}</b> "
        f"with <b>{insights['top_city_trips']:,}</b> trips."
    )

if insights.get("highest_fare_trip"):
    hf = insights["highest_fare_trip"]
    cons_txt.append(
        f"Highest fare trip: <b>{hf['trip_id']}</b> "
        f"(Fare: <b>{hf['fare']}</b>, Pickup: <b>{hf['pickup']}</b>, Drop: <b>{hf['drop']}</b>, City: <b>{hf['city']}</b>)."
    )

for p in cons_txt:
    story += [Paragraph(p, N), Spacer(1,4)]

# Build PDF
doc.build(story)
print("PDF saved:", pdf_file)

from google.colab import files
files.download("/content/Trip_Report.pdf")